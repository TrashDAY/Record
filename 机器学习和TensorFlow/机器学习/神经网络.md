#  神经网络基础知识

## 		线性函数

​			![74640882135](.\mdRes\线性函数映射.png)

​		x：像素点

​		W：该点对应权重

​				![74640913496](.\mdRes\线性函数分类.png)

​		上图假设有10个分类，W为10个分类对应的参数，x是将图片像素矩阵变为3072x1的向量，b是微调参数，最后计算出相应类别的得分矩阵10x1

​		**神经网络的工作就是不断优化W矩阵，使其能最好分类图片**



## 	损失函数

​			定义训练模型测试结果与真实值的差异，越小越好

​			损失函数相同，模型就相同？![74642426248](.\mdRes\损失函数与模型相同否.png)

​			 **答案是不一定，并且上述A、B两个模型中B更好，A过于关注局部容易产生过拟合**

​			如果模型的容量（参数量或表达能力）过高，它会倾向于记住训练数据中的局部噪声或无关细节，而非学习数据的全局规律。例如，一个超参数量大的神经网络可以完美拟合训练集（甚至达到零训练误差），但在未见过的数据上表现糟糕。

​			解决方法：**加入正则化惩罚项**![74642461018](.\mdRes\正则化惩罚项.png)

​			由上述正则化惩罚项示例可发现其与数据x无关，只与模型W有关

​			λ为惩罚系数，决定正则化惩罚项影响大小  

## 	分类器

![74642502938](.\mdRes\softmax分类器.png)

​			g函数可以将全体实数映射到（0,1）上，从而可以用概率表示得分

​			示例：softmax分类器![74642529776](.\mdRes\分类器示例.png)

​			指数函数变换：用于扩大差异

​			归一化：将数值转化为概率

​			损失函数：损失函数只关注标签对应的概率，概率越大，损失越小。所以-log函数可以表达出在(0,1]内概率（x轴）越大，损失（y轴）越小。当概率为1时，损失为0。最常用的损失函数就是-log

## 	前向传播			

​	![74642584691](.\mdRes\前向传播.png)

​			将数据从数据集经过模型W计算后得到得分（概率）和损失就是前向传播。**回归任务计算得分，分类任务计算概率**

## 	反向传播

​		梯度下降

![74642722876](.\mdRes\梯度下降.png)

​		梯度下降要符合链式法则，与求导相似。

​		某一项的梯度是本身的导数乘以从最后一项迭代传回来的导数

![74642884213](.\mdRes\梯度下降链式法则.png)

​		例子：绿色项为从前面传来的输入，红色项为这一项的梯度

![74642834017](.\mdRes\梯度下降计算.png)

​		梯度向前传播的规律

![74642900962](.\mdRes\梯度向前传播的规律.png)

​		





# 神经网络整体架构

​	![74642924623](.\mdRes\神经网络整体架构.png)

